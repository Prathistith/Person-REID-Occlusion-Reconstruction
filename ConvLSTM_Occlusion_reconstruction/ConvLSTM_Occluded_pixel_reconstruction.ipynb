{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Syn_Occ_sh_IIT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "paHFCSDWQSP3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4FbsjJGQYlq",
        "outputId": "7a50d35e-03af-468a-9026-80319b8de150"
      },
      "source": [
        "fpath = '/content/drive/MyDrive/Dataset/syn_occ_sh.npy'\n",
        "\n",
        "dataset = np.load(fpath)\n",
        "print(dataset.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(872, 5, 80, 30, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yzUya-RQrz7",
        "outputId": "3b2ed4c4-d8ab-4985-98a7-1fd108c49267"
      },
      "source": [
        "indexes = np.arange(dataset.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "train_index = indexes[: int(0.90 * dataset.shape[0])]\n",
        "val_index = indexes[int(0.90 * dataset.shape[0]) :]\n",
        "len(train_index), len(val_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 88)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiUp-O30RIXJ"
      },
      "source": [
        "train_dataset = dataset[train_index]\n",
        "val_dataset = dataset[val_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykyeCtUiRXaC"
      },
      "source": [
        "# Normalize the data to the 0-1 range.\n",
        "train_dataset = train_dataset / 255.0\n",
        "val_dataset = val_dataset / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG2ybAvFRqOj"
      },
      "source": [
        "def create_shifted_frames(data):\n",
        "    x = data[:, 0 : data.shape[1] - 2, :, :]\n",
        "    y = data[:, -1, :, :]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "x_train, y_train = create_shifted_frames(train_dataset)\n",
        "x_val, y_val = create_shifted_frames(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNXiaBImRy4J",
        "outputId": "3f64300b-03ac-4939-8e6c-9bd71c221168"
      },
      "source": [
        "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
        "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Shapes: (784, 3, 80, 30, 3), (784, 80, 30, 3)\n",
            "Validation Dataset Shapes: (88, 3, 80, 30, 3), (88, 80, 30, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4wEVhfaSKDg"
      },
      "source": [
        "inp = layers.Input(shape=(None, *x_train.shape[2:]))\n",
        "\n",
        "\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=128,\n",
        "    kernel_size=(5, 5),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(inp)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=128,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=16,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=3,\n",
        "    kernel_size=(1, 1),\n",
        "    padding=\"same\",\n",
        "    return_sequences=False,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    filters=3, kernel_size=(3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        ")(x)\n",
        "\n",
        "\n",
        "model = keras.models.Model(inp, x)\n",
        "model.compile(\n",
        "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQT8s15CTBWq",
        "outputId": "72b2b166-7b85-487d-cdae-4f6094269685"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, 80, 30, 3)] 0         \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d (ConvLSTM2D)    (None, None, 80, 30, 128) 1677312   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 80, 30, 128) 512       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_1 (ConvLSTM2D)  (None, None, 80, 30, 128) 1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 80, 30, 128) 512       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_2 (ConvLSTM2D)  (None, None, 80, 30, 64)  442624    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, None, 80, 30, 64)  256       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_3 (ConvLSTM2D)  (None, None, 80, 30, 64)  295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, None, 80, 30, 64)  256       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_4 (ConvLSTM2D)  (None, None, 80, 30, 32)  110720    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, None, 80, 30, 32)  128       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_5 (ConvLSTM2D)  (None, None, 80, 30, 32)  73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, None, 80, 30, 32)  128       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_6 (ConvLSTM2D)  (None, None, 80, 30, 32)  73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, None, 80, 30, 32)  128       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_7 (ConvLSTM2D)  (None, None, 80, 30, 16)  27712     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, None, 80, 30, 16)  64        \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_8 (ConvLSTM2D)  (None, 80, 30, 3)         240       \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 80, 30, 3)         84        \n",
            "=================================================================\n",
            "Total params: 3,883,716\n",
            "Trainable params: 3,882,724\n",
            "Non-trainable params: 992\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWq-p8duS7Tb",
        "outputId": "67261a5c-ae75-4f24-bbad-464fb8ad26fd"
      },
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "# hyperparameters.\n",
        "epochs = 150\n",
        "batch_size = 64\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "13/13 [==============================] - 59s 3s/step - loss: 0.6636 - val_loss: 0.6889\n",
            "Epoch 2/150\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.6098 - val_loss: 0.6844\n",
            "Epoch 3/150\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.5680 - val_loss: 0.6803\n",
            "Epoch 4/150\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.5511 - val_loss: 0.6795\n",
            "Epoch 5/150\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.5454 - val_loss: 0.6806\n",
            "Epoch 6/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5429 - val_loss: 0.6815\n",
            "Epoch 7/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5399 - val_loss: 0.6814\n",
            "Epoch 8/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5381 - val_loss: 0.6817\n",
            "Epoch 9/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5357 - val_loss: 0.6804\n",
            "Epoch 10/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5342 - val_loss: 0.6798\n",
            "Epoch 11/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5336 - val_loss: 0.6784\n",
            "Epoch 12/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5333 - val_loss: 0.6764\n",
            "Epoch 13/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5331 - val_loss: 0.6744\n",
            "Epoch 14/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5329 - val_loss: 0.6711\n",
            "Epoch 15/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5327 - val_loss: 0.6670\n",
            "Epoch 16/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5325 - val_loss: 0.6628\n",
            "Epoch 17/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5324 - val_loss: 0.6576\n",
            "Epoch 18/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5319 - val_loss: 0.6515\n",
            "Epoch 19/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5316 - val_loss: 0.6450\n",
            "Epoch 20/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5313 - val_loss: 0.6386\n",
            "Epoch 21/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5311 - val_loss: 0.6298\n",
            "Epoch 22/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5308 - val_loss: 0.6229\n",
            "Epoch 23/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5306 - val_loss: 0.6166\n",
            "Epoch 24/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5303 - val_loss: 0.6079\n",
            "Epoch 25/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5300 - val_loss: 0.6011\n",
            "Epoch 26/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5295 - val_loss: 0.5947\n",
            "Epoch 27/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5293 - val_loss: 0.5897\n",
            "Epoch 28/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5291 - val_loss: 0.5837\n",
            "Epoch 29/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5287 - val_loss: 0.5788\n",
            "Epoch 30/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5285 - val_loss: 0.5757\n",
            "Epoch 31/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5283 - val_loss: 0.5744\n",
            "Epoch 32/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5279 - val_loss: 0.5724\n",
            "Epoch 33/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5276 - val_loss: 0.5700\n",
            "Epoch 34/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5274 - val_loss: 0.5685\n",
            "Epoch 35/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5270 - val_loss: 0.5677\n",
            "Epoch 36/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5267 - val_loss: 0.5657\n",
            "Epoch 37/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5263 - val_loss: 0.5651\n",
            "Epoch 38/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5263 - val_loss: 0.5622\n",
            "Epoch 39/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5259 - val_loss: 0.5604\n",
            "Epoch 40/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5257 - val_loss: 0.5580\n",
            "Epoch 41/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5254 - val_loss: 0.5576\n",
            "Epoch 42/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5253 - val_loss: 0.5550\n",
            "Epoch 43/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5249 - val_loss: 0.5529\n",
            "Epoch 44/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5247 - val_loss: 0.5494\n",
            "Epoch 45/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5245 - val_loss: 0.5474\n",
            "Epoch 46/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5239 - val_loss: 0.5477\n",
            "Epoch 47/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5236 - val_loss: 0.5443\n",
            "Epoch 48/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5235 - val_loss: 0.5434\n",
            "Epoch 49/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5233 - val_loss: 0.5398\n",
            "Epoch 50/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5231 - val_loss: 0.5386\n",
            "Epoch 51/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5228 - val_loss: 0.5397\n",
            "Epoch 52/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5227 - val_loss: 0.5373\n",
            "Epoch 53/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5225 - val_loss: 0.5335\n",
            "Epoch 54/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5223 - val_loss: 0.5334\n",
            "Epoch 55/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5220 - val_loss: 0.5322\n",
            "Epoch 56/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5218 - val_loss: 0.5294\n",
            "Epoch 57/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5215 - val_loss: 0.5286\n",
            "Epoch 58/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5214 - val_loss: 0.5281\n",
            "Epoch 59/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5212 - val_loss: 0.5275\n",
            "Epoch 60/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5209 - val_loss: 0.5257\n",
            "Epoch 61/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5211 - val_loss: 0.5249\n",
            "Epoch 62/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5206 - val_loss: 0.5257\n",
            "Epoch 63/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5204 - val_loss: 0.5238\n",
            "Epoch 64/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5202 - val_loss: 0.5248\n",
            "Epoch 65/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5201 - val_loss: 0.5231\n",
            "Epoch 66/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5197 - val_loss: 0.5233\n",
            "Epoch 67/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5198 - val_loss: 0.5225\n",
            "Epoch 68/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5196 - val_loss: 0.5223\n",
            "Epoch 69/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5195 - val_loss: 0.5228\n",
            "Epoch 70/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5192 - val_loss: 0.5222\n",
            "Epoch 71/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5191 - val_loss: 0.5226\n",
            "Epoch 72/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5193 - val_loss: 0.5223\n",
            "Epoch 73/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5189 - val_loss: 0.5211\n",
            "Epoch 74/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5187 - val_loss: 0.5209\n",
            "Epoch 75/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5184 - val_loss: 0.5207\n",
            "Epoch 76/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5183 - val_loss: 0.5203\n",
            "Epoch 77/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5182 - val_loss: 0.5208\n",
            "Epoch 78/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5180 - val_loss: 0.5204\n",
            "Epoch 79/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5179 - val_loss: 0.5205\n",
            "Epoch 80/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5178 - val_loss: 0.5200\n",
            "Epoch 81/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5177 - val_loss: 0.5200\n",
            "Epoch 82/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5176 - val_loss: 0.5196\n",
            "Epoch 83/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5174 - val_loss: 0.5202\n",
            "Epoch 84/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5173 - val_loss: 0.5195\n",
            "Epoch 85/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5173 - val_loss: 0.5193\n",
            "Epoch 86/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5171 - val_loss: 0.5194\n",
            "Epoch 87/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5170 - val_loss: 0.5203\n",
            "Epoch 88/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5171 - val_loss: 0.5188\n",
            "Epoch 89/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5167 - val_loss: 0.5194\n",
            "Epoch 90/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5168 - val_loss: 0.5194\n",
            "Epoch 91/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5168 - val_loss: 0.5185\n",
            "Epoch 92/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5163 - val_loss: 0.5188\n",
            "Epoch 93/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5162 - val_loss: 0.5187\n",
            "Epoch 94/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5161 - val_loss: 0.5182\n",
            "Epoch 95/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5160 - val_loss: 0.5182\n",
            "Epoch 96/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5160 - val_loss: 0.5182\n",
            "Epoch 97/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5158 - val_loss: 0.5180\n",
            "Epoch 98/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5157 - val_loss: 0.5175\n",
            "Epoch 99/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5155 - val_loss: 0.5176\n",
            "Epoch 100/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5156 - val_loss: 0.5177\n",
            "Epoch 101/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5157 - val_loss: 0.5173\n",
            "Epoch 102/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5154 - val_loss: 0.5177\n",
            "Epoch 103/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5153 - val_loss: 0.5178\n",
            "Epoch 104/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5152 - val_loss: 0.5185\n",
            "Epoch 105/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5153 - val_loss: 0.5169\n",
            "Epoch 106/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5150 - val_loss: 0.5167\n",
            "Epoch 107/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5147 - val_loss: 0.5166\n",
            "Epoch 108/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5146 - val_loss: 0.5171\n",
            "Epoch 109/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5147 - val_loss: 0.5170\n",
            "Epoch 110/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5145 - val_loss: 0.5163\n",
            "Epoch 111/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5145 - val_loss: 0.5164\n",
            "Epoch 112/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5147 - val_loss: 0.5180\n",
            "Epoch 113/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5145 - val_loss: 0.5161\n",
            "Epoch 114/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5145 - val_loss: 0.5170\n",
            "Epoch 115/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5148 - val_loss: 0.5178\n",
            "Epoch 116/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5148 - val_loss: 0.5167\n",
            "Epoch 117/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5143 - val_loss: 0.5164\n",
            "Epoch 118/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5142 - val_loss: 0.5175\n",
            "Epoch 119/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5139 - val_loss: 0.5154\n",
            "Epoch 120/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5138 - val_loss: 0.5155\n",
            "Epoch 121/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5156\n",
            "Epoch 122/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5137 - val_loss: 0.5154\n",
            "Epoch 123/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5154\n",
            "Epoch 124/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5153\n",
            "Epoch 125/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5153\n",
            "Epoch 126/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5153\n",
            "Epoch 127/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 128/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 129/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 130/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 131/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 132/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5152\n",
            "Epoch 133/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5152\n",
            "Epoch 134/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 135/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5152\n",
            "Epoch 136/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5137 - val_loss: 0.5152\n",
            "Epoch 137/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 138/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5152\n",
            "Epoch 139/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5152\n",
            "Epoch 140/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5136 - val_loss: 0.5152\n",
            "Epoch 141/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 142/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5152\n",
            "Epoch 143/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n",
            "Epoch 144/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n",
            "Epoch 145/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n",
            "Epoch 146/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n",
            "Epoch 147/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n",
            "Epoch 148/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n",
            "Epoch 149/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n",
            "Epoch 150/150\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.5135 - val_loss: 0.5151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0b503b5d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSZOnavL8NVC"
      },
      "source": [
        "from keras.models import load_model\n",
        "m=load_model('/content/drive/MyDrive/Dataset/model_syn_occ1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkwcOWimqbmz"
      },
      "source": [
        "for _ in range(10):\n",
        "  example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n",
        "\n",
        "  frames = example[:3, ...]\n",
        "  original_frames = example[-1, ...]\n",
        "  new_prediction = m.predict(np.expand_dims(frames, axis=0))\n",
        "  s = new_prediction.reshape(80, 30, 3)\n",
        "  plt.figure(figsize=(20, 5))\n",
        "  plt.subplot(1, 5, 1),plt.imshow(frames[0]),plt.title(\"Frame 1\")\n",
        "  plt.subplot(1, 5, 2),plt.imshow(frames[1]),plt.title(\"Frame 2\")\n",
        "  plt.subplot(1, 5, 3),plt.imshow(original_frames),plt.title(\"Frame 3\")\n",
        "  plt.subplot(1, 5, 4),plt.imshow(frames[2]),plt.title(\"Occluded Frame 3\")\n",
        "  plt.subplot(1, 5, 5),plt.imshow(s),plt.title(\"LSTM Model Output\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aPDoSU4VL5_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}